{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# EU HICP Package Holidays Price Forecast - Phase 1: Data Collection & Cleaning\n",
        "\n",
        "## Overview\n",
        "This notebook implements **Phase 1** of the EU HICP Package Holidays Price Forecast project, focusing on comprehensive data collection and cleaning using the FRED, BLS, Eurostat, and ECB APIs.\n",
        "\n",
        "### Objectives\n",
        "1. **Collect HICP data** for EU and Germany package holidays (CP96EAMM, CP96DEMM)\n",
        "2. **Gather economic indicators** that influence travel demand\n",
        "3. **Fetch comparative data** from US travel markets via BLS\n",
        "4. **Clean and structure data** using polars for efficient processing\n",
        "5. **Create unified dataset** for subsequent analysis phases\n",
        "\n",
        "### Data Sources\n",
        "- **FRED (Federal Reserve Economic Data)**: HICP indices and economic indicators\n",
        "- **BLS (Bureau of Labor Statistics)**: US travel price indices for comparison\n",
        "- **Eurostat**: EU tourism statistics (future enhancement)\n",
        "- **ECB**: European Central Bank monetary indicators (future enhancement)\n",
        "\n",
        "### Key Technologies\n",
        "- **Polars**: Fast DataFrame operations and lazy evaluation\n",
        "- **NumPy**: Numerical computations\n",
        "- **Plotly**: Interactive visualizations\n",
        "- **FRED/BLS APIs**: Data collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project root to path for imports\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import project modules\n",
        "from config import validate_api_keys, HICP_SERIES, ECONOMIC_INDICATORS\n",
        "from data_collector import DataCollector\n",
        "\n",
        "# Configure polars\n",
        "pl.Config.set_tbl_rows(20)\n",
        "pl.Config.set_tbl_cols(10)\n",
        "pl.Config.set_fmt_str_lengths(50)\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")\n",
        "print(f\"Polars version: {pl.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "\n",
        "# Validate API configuration\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"API CONFIGURATION CHECK\")\n",
        "print(\"=\"*50)\n",
        "validate_api_keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 1: Initialize Data Collector and Validate Configuration\n",
        "\n",
        "Before collecting data, we'll initialize our data collector and verify that all API keys are properly configured. The system will work with missing API keys but with limited functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the data collector\n",
        "collector = DataCollector()\n",
        "\n",
        "# Display the data series we'll be collecting\n",
        "print(\"TARGET HICP SERIES:\")\n",
        "print(\"=\"*40)\n",
        "for name, series_id in HICP_SERIES.items():\n",
        "    print(f\"  {name}: {series_id}\")\n",
        "\n",
        "print(\"\\nECONOMIC INDICATORS:\")\n",
        "print(\"=\"*40)\n",
        "for name, series_id in ECONOMIC_INDICATORS.items():\n",
        "    print(f\"  {name}: {series_id}\")\n",
        "\n",
        "print(f\"\\nData collection period: 2010-01-01 to 2024-12-31\")\n",
        "print(f\"Forecast target: July 2025\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 2: Collect All Data from APIs\n",
        "\n",
        "Now we'll collect data from all available sources. This process will:\n",
        "1. **Fetch FRED data**: HICP indices and economic indicators\n",
        "2. **Collect BLS data**: US travel price indices for comparative analysis\n",
        "3. **Save raw data**: Store collected data in parquet format for efficient access\n",
        "4. **Generate summary**: Provide overview of collected datasets\n",
        "\n",
        "*Note: If you don't have API keys configured, you can still explore the notebook structure and methods.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all data from APIs\n",
        "print(\"Starting comprehensive data collection...\")\n",
        "print(\"This may take a few minutes depending on API response times.\\n\")\n",
        "\n",
        "# Collect all datasets\n",
        "datasets = collector.collect_all_data()\n",
        "\n",
        "# Save the collected data\n",
        "collector.save_data(datasets, format='parquet')\n",
        "\n",
        "# Also save as CSV for easy inspection\n",
        "collector.save_data(datasets, format='csv')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA COLLECTION COMPLETE\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 3: Data Quality Assessment and Summary\n",
        "\n",
        "Let's examine the collected data to understand its structure, coverage, and quality. This analysis will help us identify any data gaps or issues that need to be addressed in the cleaning phase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive data summary\n",
        "def analyze_dataset(df: pl.DataFrame, dataset_name: str) -> None:\n",
        "    \"\"\"Analyze a dataset and print comprehensive summary.\"\"\"\n",
        "    if df.is_empty():\n",
        "        print(f\"\\n{dataset_name.upper()} Dataset: EMPTY\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n{dataset_name.upper()} Dataset Analysis:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(f\"Rows: {len(df):,}\")\n",
        "    print(f\"Columns: {df.width}\")\n",
        "    \n",
        "    # Date range\n",
        "    if 'date' in df.columns:\n",
        "        date_min = df['date'].min()\n",
        "        date_max = df['date'].max()\n",
        "        print(f\"Date range: {date_min} to {date_max}\")\n",
        "        \n",
        "        # Calculate data frequency\n",
        "        unique_dates = df['date'].unique().sort()\n",
        "        if len(unique_dates) > 1:\n",
        "            # Estimate frequency based on first few date differences\n",
        "            date_diffs = []\n",
        "            for i in range(min(10, len(unique_dates)-1)):\n",
        "                diff = unique_dates[i+1] - unique_dates[i]\n",
        "                date_diffs.append(diff.days if hasattr(diff, 'days') else diff)\n",
        "            \n",
        "            avg_diff = np.mean(date_diffs) if date_diffs else 0\n",
        "            if 28 <= avg_diff <= 31:\n",
        "                freq = \"Monthly\"\n",
        "            elif 7 <= avg_diff <= 7:\n",
        "                freq = \"Weekly\"\n",
        "            elif avg_diff == 1:\n",
        "                freq = \"Daily\"\n",
        "            else:\n",
        "                freq = f\"~{avg_diff:.1f} days\"\n",
        "            \n",
        "            print(f\"Estimated frequency: {freq}\")\n",
        "    \n",
        "    # Series information\n",
        "    if 'series_name' in df.columns:\n",
        "        series = df['series_name'].unique().to_list()\n",
        "        print(f\"Number of series: {len(series)}\")\n",
        "        print(\"Series names:\")\n",
        "        for s in series:\n",
        "            series_count = df.filter(pl.col('series_name') == s).height\n",
        "            print(f\"  • {s}: {series_count:,} observations\")\n",
        "    \n",
        "    # Missing data analysis\n",
        "    if 'value' in df.columns:\n",
        "        total_obs = len(df)\n",
        "        missing_obs = df.filter(pl.col('value').is_null()).height\n",
        "        missing_pct = (missing_obs / total_obs) * 100 if total_obs > 0 else 0\n",
        "        \n",
        "        print(f\"\\nData Quality:\")\n",
        "        print(f\"Missing values: {missing_obs:,} ({missing_pct:.1f}%)\")\n",
        "        \n",
        "        if missing_obs < total_obs:\n",
        "            value_stats = df.filter(pl.col('value').is_not_null())['value']\n",
        "            print(f\"Value range: {value_stats.min():.2f} to {value_stats.max():.2f}\")\n",
        "            print(f\"Mean value: {value_stats.mean():.2f}\")\n",
        "    \n",
        "    # Display sample data\n",
        "    print(f\"\\nSample data (first 5 rows):\")\n",
        "    print(df.head(5))\n",
        "\n",
        "# Analyze each dataset\n",
        "for name, df in datasets.items():\n",
        "    analyze_dataset(df, name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 4: Data Cleaning and Standardization\n",
        "\n",
        "Now we'll clean and standardize the collected data using polars' efficient operations. This includes:\n",
        "\n",
        "1. **Handle missing values**: Implement appropriate imputation strategies\n",
        "2. **Standardize formats**: Ensure consistent date and value formats\n",
        "3. **Create wide format**: Pivot data for easier analysis\n",
        "4. **Add derived variables**: Calculate month-over-month changes and other features\n",
        "5. **Validate data integrity**: Check for outliers and inconsistencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_and_transform_data(datasets: dict) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean and transform collected datasets into unified format.\n",
        "    \n",
        "    Args:\n",
        "        datasets: Dictionary of raw datasets from APIs\n",
        "        \n",
        "    Returns:\n",
        "        Clean, unified polars DataFrame ready for analysis\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Starting data cleaning and transformation...\")\n",
        "    \n",
        "    # Combine all datasets\n",
        "    all_data = []\n",
        "    for source_name, df in datasets.items():\n",
        "        if df.is_empty():\n",
        "            print(f\"Skipping empty {source_name} dataset\")\n",
        "            continue\n",
        "            \n",
        "        # Add source identifier\n",
        "        df_with_source = df.with_columns([\n",
        "            pl.lit(source_name).alias('data_source')\n",
        "        ])\n",
        "        all_data.append(df_with_source)\n",
        "    \n",
        "    if not all_data:\n",
        "        print(\"Warning: No data to clean\")\n",
        "        return pl.DataFrame()\n",
        "    \n",
        "    # Combine all datasets\n",
        "    combined_df = pl.concat(all_data, how='vertical_relaxed')\n",
        "    print(f\"✓ Combined {len(all_data)} datasets into {len(combined_df):,} total observations\")\n",
        "    \n",
        "    # Data cleaning steps\n",
        "    print(\"\\nApplying data cleaning steps...\")\n",
        "    \n",
        "    # 1. Ensure proper data types and sort by date\n",
        "    cleaned_df = (\n",
        "        combined_df\n",
        "        .with_columns([\n",
        "            pl.col('date').cast(pl.Date),\n",
        "            pl.col('value').cast(pl.Float64)\n",
        "        ])\n",
        "        .sort(['series_name', 'date'])\n",
        "    )\n",
        "    \n",
        "    # 2. Handle missing values (forward fill within each series)\n",
        "    cleaned_df = (\n",
        "        cleaned_df\n",
        "        .with_columns([\n",
        "            pl.col('value').forward_fill().over('series_name').alias('value_filled')\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    # 3. Calculate month-over-month percentage changes\n",
        "    cleaned_df = (\n",
        "        cleaned_df\n",
        "        .with_columns([\n",
        "            # MoM percentage change\n",
        "            (\n",
        "                (pl.col('value_filled') / pl.col('value_filled').shift(1).over('series_name') - 1) * 100\n",
        "            ).alias('mom_pct_change'),\n",
        "            \n",
        "            # Year-over-year percentage change\n",
        "            (\n",
        "                (pl.col('value_filled') / pl.col('value_filled').shift(12).over('series_name') - 1) * 100\n",
        "            ).alias('yoy_pct_change')\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    # 4. Add temporal features\n",
        "    cleaned_df = (\n",
        "        cleaned_df\n",
        "        .with_columns([\n",
        "            pl.col('date').dt.year().alias('year'),\n",
        "            pl.col('date').dt.month().alias('month'),\n",
        "            pl.col('date').dt.quarter().alias('quarter'),\n",
        "            \n",
        "            # Seasonal indicators\n",
        "            pl.when(pl.col('date').dt.month().is_in([6, 7, 8]))\n",
        "            .then(pl.lit('Summer'))\n",
        "            .when(pl.col('date').dt.month().is_in([12, 1, 2]))\n",
        "            .then(pl.lit('Winter'))\n",
        "            .when(pl.col('date').dt.month().is_in([3, 4, 5]))\n",
        "            .then(pl.lit('Spring'))\n",
        "            .otherwise(pl.lit('Autumn'))\n",
        "            .alias('season'),\n",
        "            \n",
        "            # Holiday season indicator\n",
        "            pl.when(pl.col('date').dt.month().is_in([6, 7, 8]))\n",
        "            .then(pl.lit(True))\n",
        "            .otherwise(pl.lit(False))\n",
        "            .alias('is_holiday_season')\n",
        "        ])\n",
        "    )\n",
        "    \n",
        "    print(\"✓ Applied forward fill for missing values\")\n",
        "    print(\"✓ Calculated MoM and YoY percentage changes\")\n",
        "    print(\"✓ Added temporal and seasonal features\")\n",
        "    \n",
        "    # 5. Data quality checks\n",
        "    print(\"\\nData quality assessment:\")\n",
        "    \n",
        "    # Check for extreme outliers (beyond 3 standard deviations)\n",
        "    outlier_threshold = 3\n",
        "    for series in cleaned_df['series_name'].unique():\n",
        "        series_data = cleaned_df.filter(pl.col('series_name') == series)['mom_pct_change']\n",
        "        series_data_clean = series_data.drop_nulls()\n",
        "        \n",
        "        if len(series_data_clean) > 0:\n",
        "            mean_val = series_data_clean.mean()\n",
        "            std_val = series_data_clean.std()\n",
        "            \n",
        "            if std_val and std_val > 0:\n",
        "                outliers = series_data_clean.filter(\n",
        "                    (series_data_clean - mean_val).abs() > outlier_threshold * std_val\n",
        "                )\n",
        "                \n",
        "                if len(outliers) > 0:\n",
        "                    print(f\"  ⚠️  {series}: {len(outliers)} potential outliers detected\")\n",
        "                else:\n",
        "                    print(f\"  ✓ {series}: No extreme outliers detected\")\n",
        "    \n",
        "    return cleaned_df\n",
        "\n",
        "# Apply cleaning and transformation\n",
        "clean_data = clean_and_transform_data(datasets)\n",
        "\n",
        "# Display cleaning results\n",
        "if not clean_data.is_empty():\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"CLEANING RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Final dataset shape: {clean_data.height:,} rows × {clean_data.width} columns\")\n",
        "    print(f\"Date range: {clean_data['date'].min()} to {clean_data['date'].max()}\")\n",
        "    print(f\"Series included: {clean_data['series_name'].n_unique()}\")\n",
        "    \n",
        "    # Missing data summary\n",
        "    missing_values = clean_data['value'].null_count()\n",
        "    missing_filled = clean_data['value_filled'].null_count()\n",
        "    print(f\"Missing values (original): {missing_values:,}\")\n",
        "    print(f\"Missing values (after cleaning): {missing_filled:,}\")\n",
        "else:\n",
        "    print(\"❌ No data available for cleaning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 5: Create Wide Format Dataset for Analysis\n",
        "\n",
        "For easier analysis and modeling, we'll create a wide format version of our data where each series becomes a column. This will facilitate cross-series correlations and feature engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_wide_format_data(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert long format data to wide format for analysis.\n",
        "    \n",
        "    Args:\n",
        "        df: Clean long format DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        Wide format DataFrame with series as columns\n",
        "    \"\"\"\n",
        "    \n",
        "    if df.is_empty():\n",
        "        print(\"Cannot create wide format - no data available\")\n",
        "        return pl.DataFrame()\n",
        "    \n",
        "    print(\"Creating wide format dataset...\")\n",
        "    \n",
        "    # Select key columns and pivot\n",
        "    wide_values = (\n",
        "        df\n",
        "        .select(['date', 'series_name', 'value_filled', 'year', 'month', 'quarter', 'season', 'is_holiday_season'])\n",
        "        .unique(['date', 'series_name'])  # Remove any duplicates\n",
        "        .pivot(\n",
        "            values='value_filled',\n",
        "            index=['date', 'year', 'month', 'quarter', 'season', 'is_holiday_season'],\n",
        "            columns='series_name'\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Create wide format for MoM changes\n",
        "    wide_mom_base = (\n",
        "        df\n",
        "        .select(['date', 'series_name', 'mom_pct_change'])\n",
        "        .unique(['date', 'series_name'])\n",
        "        .pivot(\n",
        "            values='mom_pct_change',\n",
        "            index='date',\n",
        "            columns='series_name'\n",
        "        )\n",
        "    )\n",
        "    wide_mom = wide_mom_base.rename({col: f\"{col}_mom_pct\" for col in wide_mom_base.columns if col != 'date'})\n",
        "    \n",
        "    # Create wide format for YoY changes  \n",
        "    wide_yoy_base = (\n",
        "        df\n",
        "        .select(['date', 'series_name', 'yoy_pct_change'])\n",
        "        .unique(['date', 'series_name'])\n",
        "        .pivot(\n",
        "            values='yoy_pct_change',\n",
        "            index='date',\n",
        "            columns='series_name'\n",
        "        )\n",
        "    )\n",
        "    wide_yoy = wide_yoy_base.rename({col: f\"{col}_yoy_pct\" for col in wide_yoy_base.columns if col != 'date'})\n",
        "    \n",
        "    # Join all wide format datasets\n",
        "    wide_final = (\n",
        "        wide_values\n",
        "        .join(wide_mom, on='date', how='left')\n",
        "        .join(wide_yoy, on='date', how='left')\n",
        "        .sort('date')\n",
        "    )\n",
        "    \n",
        "    print(f\"✓ Created wide format dataset: {wide_final.height:,} rows × {wide_final.width} columns\")\n",
        "    \n",
        "    return wide_final\n",
        "\n",
        "# Create wide format data\n",
        "if not clean_data.is_empty():\n",
        "    wide_data = create_wide_format_data(clean_data)\n",
        "    \n",
        "    # Display sample of wide format data\n",
        "    if not wide_data.is_empty():\n",
        "        print(\"\\nSample of wide format data:\")\n",
        "        print(wide_data.head(10))\n",
        "        \n",
        "        # Summary of columns\n",
        "        print(f\"\\nColumns in wide format dataset:\")\n",
        "        for i, col in enumerate(wide_data.columns):\n",
        "            print(f\"  {i+1:2d}. {col}\")\n",
        "    \n",
        "else:\n",
        "    print(\"Cannot create wide format - no clean data available\")\n",
        "    wide_data = pl.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 6: Save Cleaned Data and Summary\n",
        "\n",
        "Let's save our cleaned and processed data for use in subsequent analysis phases and create a final summary of what we've accomplished.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned datasets\n",
        "def save_cleaned_data():\n",
        "    \"\"\"Save cleaned data in multiple formats for different use cases.\"\"\"\n",
        "    \n",
        "    print(\"Saving cleaned and processed data...\")\n",
        "    \n",
        "    if not clean_data.is_empty():\n",
        "        # Save long format (for time series analysis)\n",
        "        clean_data.write_parquet('data/clean_long_format.parquet')\n",
        "        clean_data.write_csv('data/clean_long_format.csv')\n",
        "        print(\"✓ Saved long format data\")\n",
        "    \n",
        "    if not wide_data.is_empty():\n",
        "        # Save wide format (for cross-sectional analysis)\n",
        "        wide_data.write_parquet('data/clean_wide_format.parquet')\n",
        "        wide_data.write_csv('data/clean_wide_format.csv')\n",
        "        print(\"✓ Saved wide format data\")\n",
        "    \n",
        "    # Create metadata file\n",
        "    metadata = {\n",
        "        'creation_date': datetime.now().isoformat(),\n",
        "        'data_sources': list(datasets.keys()) if datasets else [],\n",
        "        'series_collected': clean_data['series_name'].unique().to_list() if not clean_data.is_empty() else [],\n",
        "        'date_range': {\n",
        "            'start': str(clean_data['date'].min()) if not clean_data.is_empty() else None,\n",
        "            'end': str(clean_data['date'].max()) if not clean_data.is_empty() else None\n",
        "        },\n",
        "        'total_observations': len(clean_data) if not clean_data.is_empty() else 0,\n",
        "        'columns_wide_format': wide_data.columns if not wide_data.is_empty() else []\n",
        "    }\n",
        "    \n",
        "    import json\n",
        "    with open('data/data_metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(\"✓ Saved metadata file\")\n",
        "\n",
        "save_cleaned_data()\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 1 COMPLETION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not clean_data.is_empty():\n",
        "    print(f\"✓ Successfully collected and cleaned data from {len(datasets)} sources\")\n",
        "    print(f\"✓ {len(clean_data):,} total observations across {clean_data['series_name'].n_unique()} series\")\n",
        "    print(f\"✓ Date coverage: {clean_data['date'].min()} to {clean_data['date'].max()}\")\n",
        "    print(f\"✓ Added {clean_data.width - 4} derived features (MoM%, YoY%, temporal features)\")\n",
        "    print(f\"✓ Created wide format dataset with {wide_data.width} columns\")\n",
        "    \n",
        "    # Key series summary\n",
        "    print(f\"\\nKey HICP Series Status:\")\n",
        "    for series_name in ['eu_package_holidays', 'germany_package_holidays']:\n",
        "        series_data = clean_data.filter(pl.col('series_name') == series_name)\n",
        "        if not series_data.is_empty():\n",
        "            print(f\"  ✓ {series_name}: {len(series_data)} observations\")\n",
        "        else:\n",
        "            print(f\"  ❌ {series_name}: No data collected\")\n",
        "    \n",
        "    print(f\"\\nFiles Created:\")\n",
        "    print(f\"  • data/clean_long_format.parquet (primary analysis dataset)\")\n",
        "    print(f\"  • data/clean_wide_format.parquet (cross-sectional analysis)\")\n",
        "    print(f\"  • data/clean_long_format.csv (human-readable format)\")\n",
        "    print(f\"  • data/clean_wide_format.csv (human-readable format)\")\n",
        "    print(f\"  • data/data_metadata.json (dataset documentation)\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ Phase 1 completed with limited success - no data was collected\")\n",
        "    print(\"   This may be due to missing API keys or connectivity issues\")\n",
        "    print(\"   Please check your API configuration and try again\")\n",
        "\n",
        "print(f\"\\n🎯 Ready for Phase 2: Exploratory Data Analysis\")\n",
        "print(f\"   Next notebook: 02_exploratory_data_analysis.ipynb\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
